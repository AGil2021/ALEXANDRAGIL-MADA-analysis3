---
title: "RECIPES"
output: html_document
---

```{R}

install.packages("tidymodels") #fot hte recipies package and others functions
install.packages("skimr") # for variable summaries
library(skimr)
library(dplyr) #for data processing
library(here)
library(rsample)
library(recipes)
library(tidymodels)
```

```{r}
#path to data
data_location <-here::here("data","processed_data","newdata.rds")
dt<- readRDS(data_location)
```
```{r}
glimpse(dt)
```
#Split the data 
```{r}
# Put 3/4 of the data into the training set 
data_split <- initial_split(dt, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```
## New recipe to fit the categorical outcome of interest (Nausea) to all predictors
```{r}
Nausea_rec<- 
  recipe(Nausea ~ ., data = train_data)

summary(Nausea_rec) #To get the current set of variables and role
```
```{r}
# Fit a logistic model 
lr_mod <- 
  logistic_reg() %>% 
  set_engine("glm")

# Model Workflow
Nausea_wflow <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(Nausea_rec)

# Preparing the recipe and train the model from the resulting predictors:
Nausea_fit<-
  Nausea_wflow %>%
  fit(data = train_data )

#Extracting the model coefficients
Nausea_fit %>%
   extract_fit_parsnip() %>% 
   tidy()
```
# Evaluating Model
# The ROC curve provides a graphical representation of a classifierâ€™s performance
```{r}
predict(Nausea_fit, test_data )

#Predicting probabilities
Nausea_aug <-
  augment(Nausea_fit, test_data)

Nausea_aug %>%
  select(Nausea, .pred_class , .pred_No, .pred_Yes)

#ROC curve
Nausea_aug%>%
  roc_curve(truth = Nausea, .pred_No ) %>%
  autoplot()
# Getting the area under the curve
Nausea_aug%>%
  roc_auc(truth = Nausea, .pred_No )
```
#Since the ROC-AUC is 0.714 the model can considered acceptable.

#Alternative model: model that only fits the main predictor (RunnyNose) to the categorical outcome. 
```{r}
#New recipe for with Predictor = RunnyNose
Runnynose_rec<- 
  recipe(Nausea ~ RunnyNose, data = train_data)
```
```{r}

# Fit a logistic model 
lr_mod <- 
  logistic_reg() %>% 
  set_engine("glm")

# Model Workflow
Runnynose_wflow <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(Runnynose_rec)

# Preparing the recipe and train the model from the resulting predictors:
Runnynose_fit<-
  Runnynose_wflow %>%
  fit(data = train_data )

#Extracting the model coefficients
Runnynose_fit %>%
   extract_fit_parsnip() %>% 
   tidy()
```
#Evaluating Alternative Model
```{r}
predict(Runnynose_fit, test_data )

#Predicting probabilities
Runnynose_aug <-
  augment(Runnynose_fit, test_data)

#ROC curve
Runnynose_aug%>%
  roc_curve(truth = Nausea, .pred_No ) %>%
  autoplot()
# Getting the area under the curve
Runnynose_aug%>%
  roc_auc(truth = Nausea, .pred_No )
```
# Since the ROC-AUC is 0.504, we can conclude that the alternative model is not useful. 

## Amelia Foley - contribution begins

Here, we are fitting linear models to continuous outcomes

# Linear model 1 - all predictors
```{r}
#continuous outcome: body temperature
#predictors: all
bodytemp_rec<- 
  recipe(BodyTemp ~ ., data = train_data)

summary(bodytemp_rec) #To get the current set of variables and role

# fit a linear model 
lin_mod <- 
  linear_reg() %>% set_engine("lm")
  

# Model Workflow
bodytemp_wflow <-
  workflow() %>% 
  add_model(lin_mod) %>% 
  add_recipe(bodytemp_rec)

# Preparing the recipe and train the model from the resulting predictors:
bodytemp_fit<-
  bodytemp_wflow %>%
  fit(data = train_data )

# Extracting the model coefficients
bodytemp_fit %>%
   extract_fit_parsnip() %>% 
   tidy()
```

# Evaluate linear model 1
```{r}
#look at RMSE (performance metric for continous outcomes/linear models)
#we'll compare RMSE values for the test and train data. Similar values indicate a model is performing well

#used trained workflow to predict using test data
predict(bodytemp_fit, test_data)

#include probabilities
bodytemp_aug <- augment(bodytemp_fit, test_data)

bodytemp_aug #view bodytemp_aug, see .pred column at the end of the table

#get RMSE for test data
bodytemp_aug %>% rmse(truth = BodyTemp, .pred)

#get RMSE of train data
predict(bodytemp_fit, train_data) #make predictions w/ train data
bodytemptrain1_aug <- augment(bodytemp_fit, train_data) #add probabilities, predictions
bodytemptrain1_aug %>% rmse(truth = BodyTemp, .pred) #get RMSE for train data

```

The RMSE for linear model 1 test data is 1.181. The RMSE for linear model 1 train data is 1.105. These values seem very similar, but similarity can be relative. We'll compare the RMSE values for our second linear model as well. 

# Linear model 2 - main predictor
```{r}
#continuous outomce: body temperature
#main predictor: RunnyNose
bodytemp2_rec<- 
  recipe(BodyTemp ~ RunnyNose, data = train_data)

summary(bodytemp2_rec) #To get the current set of variables and role

# fit a linear model 
lin_mod <- 
  linear_reg() %>% set_engine("lm")
  

# Model Workflow
bodytemp2_wflow <-
  workflow() %>% 
  add_model(lin_mod) %>% 
  add_recipe(bodytemp2_rec)

# Preparing the recipe and train the model from the resulting predictors:
bodytemp2_fit<-
  bodytemp2_wflow %>%
  fit(data = train_data )

#Extracting the model coefficients
bodytemp2_fit %>%
   extract_fit_parsnip() %>% 
   tidy()
```
  
# Evaluate linear model 2 
```{r}
#look at RMSE (performance metric for continous outcomes/linear models)
#we'll compare RMSE values for the test and train data. Similar values indicate a model is performing well

#used trained workflow to predict using test data
predict(bodytemp2_fit, test_data)

#include probabilities
bodytemp2_aug <- augment(bodytemp2_fit, test_data)

bodytemp2_aug #view bodytemp_aug, see .pred column at the end of the table

#get RMSE of test data
bodytemp2_aug %>% rmse(truth = BodyTemp, .pred)

#get RMSE of train data
predict(bodytemp2_fit, train_data) #make predictions w/ train data
bodytemptrain_aug <- augment(bodytemp2_fit, train_data) #add probabilities, predictions
bodytemptrain_aug %>% rmse(truth = BodyTemp, .pred) #get RMSE for train data


```

Here, we see that the RMSE for linear model 2 test data is 1.22, and the RMSE for linear model 2 train data is 1.177. This still seems like a pretty small difference, indicating that the model is performing well. 

The difference between RMSE values for linear model 1 was about 0.08. The difference between RMSE values for linear model 2 was about 0.05. These values are very close in proximity, which leads me to think the models are equally fitting. However, if we were to select one model out of the two linear models, we might consider linear model 2. Since it requires only one predictor but produces similar results to the model requiring all predictors, this would be applicable and informative in clinical settings/public health applications. 

        
