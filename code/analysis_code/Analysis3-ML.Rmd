---
title: "Analysis3-ML"
author: "Alexandra Gil"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
install.packages("tidymodels") #fot hte recipies package and others functions
install.packages("skimr") # for variable summaries
install.packages("rpart")
install.packages("glmnet")
install.packages("ranger")
```
```{r}
library(skimr)
library(dplyr) #for data processing
library(here)
library(rsample)
library(recipes)
library(tidymodels)
library(rpart.plot)  # for visualizing a decision tree
library(vip) 
```

```{r}
#path to data
data_location <-here::here("data","processed_data","newdata.rds")
dt2<- readRDS(data_location)
```
```{r}
glimpse(dt2)
```
#Pre-processing
#1. Feature/Variable removal:Removing Y/N Variables, such as Weakness, Cough and Myalgia
```{r}
dt2<- dt2%>%
  select(-c(CoughYN, WeaknessYN, CoughYN2, MyalgiaYN ))
```
```{r}
#ordered factors for Weakness, CoughIntensity, Myalgia 

dt2 <- mutate(dt2, Weakness = factor(Weakness, levels = c("None", "Mild","Moderate","Severe"),ordered = TRUE))

dt2 <- mutate(dt2, CoughIntensity= factor(CoughIntensity, levels = c("None", "Mild","Moderate","Severe"),ordered = TRUE))

dt2 <- mutate(dt2, Myalgia = factor(Myalgia , levels = c("None", "Mild","Moderate","Severe"),ordered = TRUE))
```
#Removing unbalanced predictors: remove those binary predictors that have <50 entries in one category 
```{r}
countYes = function(v){length(v[v=="Yes"])}
yesses = sapply(dt2,countYes)
yesses
```
```{r}
dt2<-select(dt2,-c(Hearing, Vision))

glimpse(dt2)
```
#Data Setup
#1. Setting the seed
```{r}
set.seed(123)
```
#Split the data 
```{r}
# Put 70% of the data into the training set and 30% into testing using strata=BodyTemp
data_split <- initial_split(dt2, prop = 0.7, strata = BodyTemp)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
#Cross-Validation Folds

folds<- vfold_cv(train_data, v = 5, repeats = 5, strata = BodyTemp)
```
#Recipe for the data and fitting
```{r}
BodyTemp_rec<-recipe(BodyTemp ~ ., data = train_data) %>%
              step_dummy(all_nominal_predictors())
```
#Null model performance
```{r}
# Fit a logistic model 
ln_mod <- 
  linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")

#Recipe for a null model - Training Data
null_rec_Train<-recipe(BodyTemp ~ ., data = train_data) 

# Null Model Workflow - Training Data
null_wflow <-
  workflow() %>% 
  add_model(lm_mod) %>% 
  add_recipe(null_rec_Train)

null_fit<-
  null_wflow %>%
  fit(data = train_data )
  
# Predictions base on null model
prediction_null<-predict(null_fit, train_data)

null_aug <- augment(null_fit, train_data)

null_aug %>% select(BodyTemp)

# Extracting the model coefficients
null_fit %>%
   extract_fit_parsnip() %>% 
   tidy()
# Getting Root RMSE
rmse_traindata <-null_aug%>%
  rmse(truth = BodyTemp, .pred)

rmse_traindata
#RMSE= 1.102

##### Predicting using Test_data
prediction_null_testdata<-predict(null_fit, test_data)

null_aug_testdata <- augment(null_fit, test_data)

null_aug_testdata %>% select(BodyTemp)

# Getting Root RMSE
rmse_testdata <-null_aug_testdata%>%
  rmse(truth = BodyTemp, .pred)

rmse_testdata
#RMSE= 1.199

```

#Model tuning and fitting
##### FITTING A TREE
```{r}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
tune_spec
#tuning grid specification
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
tree_grid
#cross validation
set.seed(123)
cell_folds <- vfold_cv(train_data)
#workflow
set.seed(123)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(BodyTemp_rec) # Recipe from line 82

#model tuning with `tune_grid()`
tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = cell_folds,
    grid = tree_grid
    )
tree_res %>% collect_metrics()
```
#Ploting Metrics
```{r}
tree_res %>% autoplot()
```
#Selecting best TREE and Fitting our model
```{r}
#Selecting the best Tree base on RSME
best_tree <- tree_res %>%
  select_best("rmse")
best_tree

# Finalizing the model with the best Tree
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)
#Fit the model
tree_fit <- final_wf %>%
  fit(train_data) 
tree_fit
```
```{r}
####More ploting
final_fit %>%
  collect_metrics()

final_fit %>%
  collect_predictions() %>% 
  roc_curve(class, .pred_PS) %>% 
  autoplot()
```
